# Documentation

* [Installation](#installation)
* [Prerequisites](#prerequisites)
* [Usage](#usage)
    * [mpiBWAIdx](#mpibwaidx)
    * [mpiBWA](#mpibwa)
    * [mpiBWAByChr](#mpibwabychr)
    * [Input](#input)
    * [Options](#options)
    * [Output](#output)
* [Informatic resources](#informatic-resources)
    * [Memory](#memory)
    * [Cpu](#cpu)
    * [Benchmark](#bench)
* [Examples](#examples)
    * [Standard](#standard)
    * [Slurm](#slurm)
    * [PBS/Torque](#pbstorque)
* [Performance](#performance)
* [Parallel filesystems](#parallel-filesystems)
* [Algorithm](#algorithm)
* [References](#references)


## Installation

Follow the [Installation](INSTALL.md) guidelines to compile and install `mpiBWA`.

## Prerequisites

As `mpiBWA` relies on the Message Passing Interface (MPI) standard, `mpirun` must be available to run the program. Several MPI implementations exist
such as [mpich](https://www.mpich.org/), [open-mpi](https://www.open-mpi.org/) or [Intel® MPI Library](https://software.intel.com/en-us/mpi-library). The `mpirun` program must be available in your PATH.

### Install mpirun on CentOS

* for [open-mpi](https://www.open-mpi.org/): `sudo yum install openmpi openmpi-devel`
* for [mpich](https://www.mpich.org/): `sudo yum install mpich`

### Install mpirun on Ubuntu

* for [open-mpi](https://www.open-mpi.org/): `sudo apt-get install openmpi-bin`
* for [mpich](https://www.mpich.org/): `sudo apt-get install mpich`

## Usage

We have 2 versions of `mpiBWA`:

1. the first version creates a unique SAM file and is available in the binary `mpiBWA`
2. the second version sorts the output by the chromosomes present in the header and is available in the binary `mpiBWAByChr`


After installation, 3 binaries are created:
1. `mpiBWA`
2. `mpiBWAByChr`
3. `mpiBWAIdx`

### mpiBWAIdx

`mpiBWAIdx` is responsible for creating a binary image of the reference genome. This image is subsequently loaded in the shared memory by `mpiBWA`. Then, every `mpiBWA` process on the same computing node will share the same genome reference in order to save memory usage. `mpiBWAIdx` does not need MPI to run. To create a binary image from a reference genome type:

`mpiBWAIdx myReferenceGenome.fa`

This creates the file `myReferenceGenome.fa.map`.

Importantly, `mpiBWAIdx` requires the following files: `myReferenceGenome.fa.sa`, `myReferenceGenome.fa.bwt`, `myReferenceGenome.fa.ann`, `myReferenceGenome.fa.pac`, `myReferenceGenome.fa.amb` to build the `fa.map` file. These files are generated by `bwa index` (see [bwa documentation](http://bio-bwa.sourceforge.net/bwa.shtml)). It works also with fasta file.

The `.map` file needs only to be generated once.

The [genome.sh](../examples/genome.sh) script provides an example to build the `.map` file.

### mpiBWA

`mpiBWA` is executed with the `mpirun` program, for example:

`mpirun -n 2 mpiBWA mem -t 8 -o ./HCC1187C.sam hg19.small.fa examples/data/HCC1187C_R1_10K.fastq examples/data/HCC1187C_R2_10K.fastq`

This command launches 2 processes MPI and 8 threads will be created by mpiBWA.

WARNING: do not write the extension `.map` for the reference. If the file is `myReferenceGenome.fa.map` just provide in the command line `myReferenceGenome.fa` to `mpiBWA`.

The `-n` options passed to `mpirun` indicates the number of processes to run in parallele. The total number of cores required is the product of the values provided in the `-n` and `-t` options (in the previous example, 16 cores are required). For more details on how to choose the number processes, see the [Informatic resources](#informatic-resources) section.

`mpiBWA` requires several mandatory arguments:

* [Input](#input): a reference genome in `.map` format, one or two fastq files trimmed or not
* [Output](#output): a sam file containing the entire reference header (e.g. HCC1187C.sam)

### mpiBWAByChr

If you want to split the results of the alignment by chromosome, use `mpiBWAByChr`, for example:

`mpirun -n 2 mpiBWAByChr mem -t 8 -o ./HCC1187C.sam hg19.small.fa examples/data/HCC1187C_R1_10K.fastq examples/data/HCC1187C_R2_10K.fastq`

`mpiBWAByChr` requires several mandatory arguments:

* [Input](#input): a reference genome in `.map` format, one or two fastq files trimmed or not
* [Output](#output): a sam file containing the entire reference header, sam files by chromosome, discordant.sam and unmapped.sam.
	

### Input

* a reference genome in `.map` format (but do not write the extension '.map' for the reference, e.g. myReferenceGenome.fa)
* fastq file for R1 (trimmed or not)
* fastq file for R2 (trimmed or not) if the data come from paired-end sequencing (optional)

### Options

* `bwa mem` options can be passed from command line (e.g. `mpiBWA mem -t 8 -k 18`)

### output

In the case of `mpiBWA`:

* a sam file containing the entire reference header

In the case of `mpiBWAByChr`, additional files are provided:

* Individual chrN.sam files with aligned reads on each chrN (ChrN are the chromosome name from the header). The chrN.sam contains the header for the chrN and the reads mapping to that chromosome. The file contains primary and supplementary alignments for a read. If supplementary mapping are discordant they are not filtered out. The file can be sorted independently with [mpiSORT](https://github.com/bioinfo-pf-curie/mpiSORT) and during the sorting these supplementary alignments are filtered out in the discordant.gz file (see [mpiSORT](https://github.com/bioinfo-pf-curie/mpiSORT) documentation).

* The file discordant.sam contains primary chimeric alignments with their secondary alignments.
* The unmapped.sam contains unmapped reads.

Note that:

1. Supplementary or secondary alignments could be ignored with the `-M` [bwa-mem](http://bio-bwa.sourceforge.net/bwa.shtml) option passed to `mpiBWA`. In all cases they are filtered out by [mpiSORT](https://github.com/bioinfo-pf-curie/mpiSORT).

2. The discordant fragments are not extracted from the chromosome files, they are just copied.
The purpose of the discordant fragment SAM is to help the marking of duplicates in the downstream analysis.
Indeed it is easier to mark them separately and pass the result in the chromosome file.

## Informatic resources

### Memory

The total memory used during the alignment if approximately the size of the `.map` file plus the size of the SAM chunks loaded by each bwa tasks. A bwa thread takes around 300 MB of memory.
 
### Cpu

The number of cores is related to the number of rank of the MPI jobs and to the number of threads you ask with bwa with the -t option. For example, the command line `mpirun -n 4 mpiBWA mem -t 8 -o HCC1187C.sam hg19.small.fa examples/data/HCC1187C_R1_10K.fastq examples/data/HCC1187C_R2_10K.fastq` will use 32 cores.

### Benchmark

This part is very important read carefully this section before running mpiBWA on your cluster.

In this section we present a guideline to benchmark mpiBWA with your infrastructure.
We will answer questions about how to use efficiently multithreading with MPI.
We present here a example we did on our architecture at Institut Curie.
During the benchmark make sure you are alone on the nodes.  

First we set the baselines.

* BWA MEM baselines

1 threads on 1 node : `bwa mem -t 1`  
 [M::mem_process_seqs] Processed 40246 reads in 23.303 CPU sec, 23.367 real sec

8 threads on 1 node : `bwa mem -t 8`  
[M::mem_process_seqs] Processed 322302 reads in 199.261 CPU sec, 25.104 real sec

16 threads on 1 node : `bwa mem -t 16`  
[M::mem_process_seqs] Processed 644448 reads in 413.054 CPU sec, 26.000 real sec

Now we compare those base lines with mpiBWA  

* mpiBWA MEM baselines 

We start with one node one thread: 

1 threads on 1 node : `mpirun -n 1 mpiBWA mem -t 1`  
[M::mem_process_seqs] Processed 40224 reads in 25.779 CPU sec, 25.840 real sec

and on several nodes:

1 threads on 8 nodes : `mpirun -N 8 -npernode 1 -n 8 mpiBWA mem -t 1`  
[M::mem_process_seqs] Processed 40244 reads in 24.416 CPU sec, 24.475 real sec

So far we don’t see differences compare with BWA mem baseline 

Now we procede further with the parallelization. We start to increase the number of mpi jobs with 10 bwa threads

10 threads on 1 node : `mpirun -N 1 -n 1 mpiBWA mem -t 10`  
[M::mem_process_seqs] Processed 402610 reads in 257.005 CPU sec, 25.803 real sec

20 threads on 1 node :  `mpirun -N 1 -n 1 mpiBWA mem -t 20`  
[M::mem_process_seqs] Processed 804856 reads in 546.449 CPU sec, 27.477 real sec

And we increase the number of nodes

20 threads on 2 node : `mpirun -N 2 -npernode 1 -n 2 --bind-to socket mpiBWA mem -t 10`  
[M::mem_process_seqs] Processed 403144 reads in 260.081 CPU sec, 26.114 real sec


40 threads on 2 node : `mpirun -N 2 -npernode 1 -n 2 --bind-to socket mpiBWA mem -t 20`  
[M::mem_process_seqs] Processed 805198 reads in 549.086 CPU sec, 27.610 real sec

So we have no differences if we execute on 1 node and on 2 nodes. We can repeat with 3 and more nodes.  
Test also the setup with mpiBWAByChr.

Conclusion:
 
With our configuration running mpiBWA with 10 threads seems the best option.   
We notice a small increase when we use all the cores of a node. We recommand to leave some cores for the system node.  
Explore the mpirun options as we do with the bind to socket, this could help and remove contention like NUMA effects.          

## Examples

There are many ways to distribute and bind MPI jobs according to your architecture. We provide below several examples to launch MPI jobs in a standard manner or with a job scheduling system such as [Slurm](https://slurm.schedmd.com/sbatch.html) and [PBS/Torque](https://support.adaptivecomputing.com/support/documentation-index/torque-resource-manager-documentation/).

Two toy datasets (fastq files trimmed or not) are provided in the [examples/data](../examples/data) folder for testing the program.

### Standard

`mpirun` can be launched in a standard manner without using any job scheduling systems. For example:

`mpirun -n 4 mpiBWA mem -t 8 -o ${HOME}/mpiBWAExample/HCC1187C.sam ${HOME}/mpiBWAExample/hg19.small.fa examples/data/HCC1187C_R1_10K.fastq examples/data/HCC1187C_R2_10K.fastq` 

If needed, a file with the server name in `-host` option can be provided to `mpirun`. We invite you to read the `mpirun` documentation for more details.

You can go in the [examples](../examples) directory and execute the [standard.sh](../examples/standard.sh) script to test the program.

### Slurm

In order to submit a job using [Slurm](https://slurm.schedmd.com/sbatch.html), you can write a shell script as follows:

```shell
#! /bin/bash
#SBATCH -J MPIBWA_32_JOBS
#SBATCH -N 2                            # Ask 2 nodes
#SBATCH -n 32                           # total number of mpi jobs 
#SBATCH -c 16                           # use 16 cores per mpi job
#SBATCH --tasks-per-node=16             # Ask 16 mpi jobs per node
#SBATCH --mem-per-cpu=${MEM}            # See Memory ressources
#SBATCH -t 01:00:00
#SBATCH -o STDOUT_FILE.%j.o
#SBATCH -e STDERR_FILE.%j.e

mpirun mpiBWA mem -t 16 -o ${HOME}/mpiBWAExample/HCC1187C.sam ${HOME}/mpiBWAExample/hg19.small.fa examples/data/HCC1187C_R1_10K.fastq examples/data/HCC1187C_R2_10K.fastq`

```

You can go in the [examples](../examples) directory and submit the job with `sbatch` using the [slurm.sh](../examples/slurm.sh) script to test the program.

### PBS/Torque

In order to submit a job using [PBS/Torque](https://support.adaptivecomputing.com/support/documentation-index/torque-resource-manager-documentation/), you can write a shell script as follows:


```shell
#! /bin/bash
#PBS -N MPIBWA_32_JOBS
#PBS -l nodes=2:ppn=16:mem=${MEM}        # Ask 2 nodes and 16 jobs per node
#PBS -l walltime=24:00:00
#PBS -o STDOUT_FILE.%j.o
#PBS -e STDERR_FILE.%j.e

mpirun mpiBWA mem -t 16 -o ${HOME}/mpiBWAExample/HCC1187C.sam ${HOME}/mpiBWAExample/hg19.small.fa examples/data/HCC1187C_R1_10K.fastq examples/data/HCC1187C_R2_10K.fastq`

```

You can go in the [examples](../examples) directory and submit the job with `qsub` command using the [pbs.sh](../examples/pbs.sh) script to test the program.

## Parallel filesystems

This software needs a parallel filesystem for execution. The program has been tested with  [Lustre](http://lustre.org/) and [BeeGFS](https://www.beegfs.io/).


WARNING: be aware that the flock mode (i.e. file locking) on parallel filesystems ([Lustre](http://lustre.org/), [BeeGFS](https://www.beegfs.io/), etc) must be on,  otherwise the reproducibility is not guaranteed.


## Algorithm

The algorithm consists of 3 parts:

1. MPI jobs create chunks of reads. All these chunks contain the same number of nucleotids.
2. MPI jobs calls aligner jobs. This part invokes BWA MEM algorithm.
3. MPI jobs write the alignment results in a SAM file or in individual chromosome SAM files. This part uses shared file pointers.

## References

This work is based on the original bwa aligner ([Burrow-Wheeler Aligner for short-read alignment](https://github.com/lh3/bwa)) written by Li et al.:

* Li H. and Durbin R. [Fast and accurate long-read alignment with Burrows-Wheeler transform](https://academic.oup.com/bioinformatics/article/26/5/589/211735). Bioinformatics, 2010 ([pdf](https://academic.oup.com/bioinformatics/article-pdf/26/5/589/16896917/btp698.pdf)).

* Li H. [Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM](https://arxiv.org/abs/1303.3997). arXiv:1303.3997v1, 2013 ([pdf](https://arxiv.org/pdf/1303.3997)).

Shared memory with MPI:

* Latham R. et al. [Implementing MPI-IO Atomic Mode and Shared File Pointers Using MPI One-Sided Communication](https://journals.sagepub.com/doi/abs/10.1177/1094342007077859), 2007.

